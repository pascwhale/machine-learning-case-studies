{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CIFAR-10 Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The CIFAR-10 dataset contains colour images that can be classified in 10 classes:\r\n",
    "\r\n",
    "- airplane\t\t\t\t\t\t\t\t\t\t\r\n",
    "- automobile\t\t\t\t\t\t\t\t\t\t\r\n",
    "- bird\t\t\t\t\t\t\t\t\t\t\r\n",
    "- cat\t\t\t\t\t\t\t\t\t\t\r\n",
    "- deer\t\t\t\t\t\t\t\t\t\t\r\n",
    "- dog\t\t\t\t\t\t\t\t\t\t\r\n",
    "- frog\t\t\t\t\t\t\t\t\t\t\r\n",
    "- horse\t\t\t\t\t\t\t\t\t\t\r\n",
    "- ship\t\t\t\t\t\t\t\t\t\t\r\n",
    "- truck"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Manage Imports\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import sklearn.preprocessing\r\n",
    "\r\n",
    "# Manage global variables\r\n",
    "NUMBER_OF_FILES = 5\r\n",
    "IMAGES_PER_FILE = 10000\r\n",
    "IMAGE_SIZE = 32\r\n",
    "NUMBER_OF_CHANNELS = 3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def unpickle(file):\r\n",
    "    import pickle\r\n",
    "    with open(file, 'rb') as fo:\r\n",
    "        dict = pickle.load(fo, encoding='bytes')\r\n",
    "    return dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metadata = unpickle(f'../../datasets/image/cifar-10-batches-py/batches.meta')\r\n",
    "for key, value in metadata.items():\r\n",
    "    print(f'{key=}, {value}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train = np.zeros((NUMBER_OF_FILES * IMAGES_PER_FILE, IMAGE_SIZE*IMAGE_SIZE*NUMBER_OF_CHANNELS), dtype=np.uint8)\r\n",
    "y_train = np.zeros((NUMBER_OF_FILES*IMAGES_PER_FILE), dtype=np.int32)\r\n",
    "\r\n",
    "for i in range(5):\r\n",
    "    batch = unpickle(f'../../datasets/image/cifar-10-batches-py/data_batch_{i+1}')\r\n",
    "    X_train[i*10000:(i+1)*10000] = np.array(batch[b'data'])\r\n",
    "    y_train[i*10000:(i+1)*10000] = np.array(batch[b'labels'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "test_batch = unpickle(f'../../datasets/image/cifar-10-batches-py/test_batch')\r\n",
    "X_test_and_val = np.array(batch[b'data'])\r\n",
    "y_test_and_val = np.array(batch[b'labels'])\r\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_and_val, y_test_and_val, \r\n",
    "                                                        train_size=0.5,\r\n",
    "                                                        random_state=0)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following block prints the shape and column datatypes of the image dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'{X_train.shape=}, {X_train.dtype=}')\r\n",
    "print(f'{y_train.shape=}, {y_train.dtype=}')\r\n",
    "print(f'{X_test.shape=}, {X_test.dtype=}')\r\n",
    "print(f'{y_test.shape=}, {y_test.dtype=}')\r\n",
    "print(f'{X_val.shape=}, {X_val.dtype=}')\r\n",
    "print(f'{y_val.shape=}, {y_val.dtype=}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following block shows how to preview an image. It is important to note that a single CIFAR image is stored in (N,C,H,W) format."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "plt.figure()\r\n",
    "plt.imshow(X_train[0].reshape(IMAGE_SIZE, IMAGE_SIZE, NUMBER_OF_CHANNELS))\r\n",
    "plt.figure()\r\n",
    "plt.imshow(X_train[0].reshape(NUMBER_OF_CHANNELS, IMAGE_SIZE, IMAGE_SIZE).transpose(1, 2, 0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training on Multiple Classifiers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Manage imports\r\n",
    "import sklearn.tree\r\n",
    "import torch\r\n",
    "import torchvision\r\n",
    "from utilities import train_estimators, plot_estimator_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Tree Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "adjusted_parameter = 'max_depth'\r\n",
    "adjusted_parameter_values = [1, 5, 10, 20, 50, 100]\r\n",
    "\r\n",
    "DecisionTreeEstimators = train_estimators(X_train, y_train,\r\n",
    "                                            sklearn.tree.DecisionTreeClassifier,\r\n",
    "                                            adjusted_parameter, adjusted_parameter_values,\r\n",
    "                                            splitter='random',\r\n",
    "                                            random_state=0)\r\n",
    "plot_estimator_scores(DecisionTreeEstimators,\r\n",
    "                        adjusted_parameter, adjusted_parameter_values,\r\n",
    "                        X_train, y_train, X_test, y_test, X_val, y_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network Classification Through a Convolutional Neural Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train_torch = torch.tensor(X_train, dtype=torch.uint8)\r\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.int32)\r\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.uint8)\r\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.int32)\r\n",
    "X_val_torch = torch.tensor(X_val, dtype=torch.uint8)\r\n",
    "y_val_torch = torch.tensor(y_val, dtype=torch.int32)\r\n",
    "\r\n",
    "print(X_train_torch.dtype)\r\n",
    "print(y_train_torch.dtype)\r\n",
    "print(X_test_torch.dtype)\r\n",
    "print(y_test_torch.dtype)\r\n",
    "print(X_val_torch.dtype)\r\n",
    "print(y_val_torch.dtype)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.manual_seed(0) # Ensure model weights initialized with same random numbers\r\n",
    "\r\n",
    "# Use 100 training samples at a time to compute the gradient.\r\n",
    "batch_size = 1\r\n",
    "\r\n",
    "# Create an object that holds a sequence of layers and activation functions\r\n",
    "model = torch.nn.Sequential(\r\n",
    "    torch.nn.Conv2d(NUMBER_OF_CHANNELS, batch_size, IMAGE_SIZE, IMAGE_SIZE),\r\n",
    ")\r\n",
    "\r\n",
    "# Create an object that can compute \"negative log likelihood of a softmax\"\r\n",
    "loss = torch.nn.CrossEntropyLoss()\r\n",
    "# Use stochastic gradient descent to train the model\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n",
    "\r\n",
    "# Make 10 passes over the training data, each time using batch_size samples to compute\r\n",
    "num_epoch = 10\r\n",
    "next_epoch = 1\r\n",
    "\r\n",
    "for epoch in range(next_epoch, next_epoch+num_epoch):\r\n",
    "    \r\n",
    "    # Make an entire pass (an 'epoch') over the training data in batch_size chunks\r\n",
    "    for i in range(0, len(X_train_torch), batch_size):        \r\n",
    "        X = X_train_torch[i:i+batch_size].reshape((batch_size, NUMBER_OF_CHANNELS, IMAGE_SIZE, IMAGE_SIZE))  # Slice out a mini-batch of features\r\n",
    "        y = y_train_torch[i:i+batch_size]     # Slice out a mini-batch of targets\r\n",
    "\r\n",
    "        y_pred = model(X)                   # Make predictions (final-layer activations)\r\n",
    "        l = loss(y_pred, y_train_torch)     # Compute loss with respect to predictions\r\n",
    "        \r\n",
    "        model.zero_grad()                   # Reset all gradient accumulators to zero (PyTorch thing)\r\n",
    "        l.backward()                        # Compute gradient of loss wrt all parameters (backprop!)\r\n",
    "        optimizer.step()                    # Use the gradients to take a step with SGD.\r\n",
    "        \r\n",
    "    print(\"Epoch %2d: loss on final training batch: %.4f\" % (epoch, l.item()))\r\n",
    "    \r\n",
    "print(\"Epoch %2d: loss on test set: %.4f\" % (epoch, loss(model(X_test_torch), y_test_torch)))\r\n",
    "next_epoch = epoch+1"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('comp432': conda)"
  },
  "interpreter": {
   "hash": "6151f332f26dcde9b4e9a2d5bb7904271ea54c2018786e9b563d2c471a54dc4f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SGEMM GPU Kernel Performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The SGEMM GPU Kernel Performance dataset measures the running time of a product between two matrices of size 2048 x 2048 using a parametrizable SGEMM GPU kernal with 241600 possible parameter contributions. Each of the combinations are representated by a single row in the dataset, along with 4 test runtimes in the last four columns.\r\n",
    "\r\n",
    "The column attributes are as follows:\r\n",
    "\r\n",
    "| Column Name | Notation | Range | Description |\r\n",
    "| - | - | - | - |\r\n",
    "| mwg | MWG | {16, 32, 64, 128} | Per-matrix 2D tiling at workgroup level |\r\n",
    "| nwg | NWG | {16, 32, 64, 128} | Per-matrix 2D tiling at workgroup level |\r\n",
    "| kwg | KWG | {16, 32} | Inner dimension of 2D tiling at workgroup level |\r\n",
    "| mdimc | MDIMC | {8, 16, 32} | Local workgroup size |\r\n",
    "| ndimc | NDIMC | {8, 16, 32} | Local workgroup size |\r\n",
    "| mdima | MDIMA | {8, 16, 32} | Local memory shape |\r\n",
    "| ndimb | NDIMB | {8, 16, 32} | Local memory shape |\r\n",
    "| kwi | KWI | {2, 8} | Kernel loop unrolling factor |\r\n",
    "| vwm | VWM | {1, 2, 4, 8} | Per-matrix vector widths for loading and storing |\r\n",
    "| vwn | VWN | {1, 2, 4, 8} | Per-matrix vector widths for loading and storing |\r\n",
    "| strm | STRM | {0, 1} | Enabling of stride for accessing off-chip memory within a single thread |\r\n",
    "| strn | STRN | {0, 1} | Enabling of stride for accessing off-chip memory within a single thread |\r\n",
    "| sa | SA | {0, 1} | Per-matrix manual caching of the 2D workgroup tile |\r\n",
    "| sb | SB | {0, 1} | Per-matrix manual caching of the 2D workgroup tile |\r\n",
    "| run1 | - | - | Run 1 result in milliseconds |\r\n",
    "| run2 | - | - | Run 2 result in milliseconds |\r\n",
    "| run3 | - | - | Run 3 result in milliseconds |\r\n",
    "| run4 | - | - | Run 4 result in milliseconds |\r\n",
    "\r\n",
    "[Source](http://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "column_names = ['mwg',\r\n",
    "                'nwg',\r\n",
    "                'kwg',\r\n",
    "                'mdimc',\r\n",
    "                'ndimc',\r\n",
    "                'mdima',\r\n",
    "                'ndimb',\r\n",
    "                'kwi',\r\n",
    "                'vwm',\r\n",
    "                'vwn',\r\n",
    "                'strm',\r\n",
    "                'strn',\r\n",
    "                'sa',\r\n",
    "                'sb',\r\n",
    "                'run1',\r\n",
    "                'run2',\r\n",
    "                'run3',\r\n",
    "                'run4']\r\n",
    "\r\n",
    "features = column_names[:-4]\r\n",
    "target = 'target'\r\n",
    "\r\n",
    "with open(\"../../datasets/regression/sgemm_product.csv\", \"r\") as dataset_file:\r\n",
    "    raw_data = pd.read_csv(dataset_file, delimiter=',', header=0, names=column_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "prepared_data = raw_data\r\n",
    "prepared_data['target'] = prepared_data[column_names[-4:]].mean(axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following block prints the shape and column datatypes of the processed dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(prepared_data.shape)\r\n",
    "print(prepared_data.dtypes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(241600, 19)\n",
      "mwg         int64\n",
      "nwg         int64\n",
      "kwg         int64\n",
      "mdimc       int64\n",
      "ndimc       int64\n",
      "mdima       int64\n",
      "ndimb       int64\n",
      "kwi         int64\n",
      "vwm         int64\n",
      "vwn         int64\n",
      "strm        int64\n",
      "strn        int64\n",
      "sa          int64\n",
      "sb          int64\n",
      "run1      float64\n",
      "run2      float64\n",
      "run3      float64\n",
      "run4      float64\n",
      "target    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "X_full = prepared_data[features].copy()\r\n",
    "y_full = prepared_data[target].copy()\r\n",
    "\r\n",
    "# Split the dataset into two parts, one part training, the other, testing and validating.\r\n",
    "X_train, X_test_and_val, y_train, y_test_and_val = train_test_split(X_full, y_full, \r\n",
    "                                                        train_size=0.6,\r\n",
    "                                                        random_state=0)\r\n",
    "# Split the second part from the previous split into two parts, one part testing, the other, validating.\r\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_and_val, y_test_and_val, \r\n",
    "                                                        train_size=0.5,\r\n",
    "                                                        random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "scaler = StandardScaler()\r\n",
    "# Fit scaler to data then transform it.\r\n",
    "X_train_scaled = scaler.fit_transform(X_train)\r\n",
    "\r\n",
    "# Apply same transformation to test and validation data without fitting.\r\n",
    "X_test_scaled = scaler.transform(X_test)\r\n",
    "X_val_scaled = scaler.transform(X_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training on Multiple Regressors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#Manage imports \r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.tree import DecisionTreeRegressor\r\n",
    "from sklearn.svm import SVR\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.ensemble import AdaBoostRegressor\r\n",
    "from sklearn.neighbors import KNeighborsRegressor\r\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\r\n",
    "from sklearn.neural_network import MLPRegressor\r\n",
    "from utilities import train_estimators, plot_estimator_scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LinearRegressionEstimator = LinearRegression().fit(X_train_scaled, y_train)\r\n",
    "LR_train_score = LinearRegressionEstimator.score(X_train_scaled, y_train)\r\n",
    "LR_test_score = LinearRegressionEstimator.score(X_test_scaled, y_test)\r\n",
    "LR_val_score = LinearRegressionEstimator.score(X_val_scaled, y_val)\r\n",
    "print(f'{LR_train_score=}, {LR_test_score=}, {LR_val_score=}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVM Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "adjusted_parameter = 'C'\r\n",
    "adjusted_parameter_values = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\r\n",
    "\r\n",
    "SVMEstimators = train_estimators(X_train_scaled, y_train,\r\n",
    "                                    SVR,\r\n",
    "                                    adjusted_parameter, adjusted_parameter_values,\r\n",
    "                                    gamma=0.0001,\r\n",
    "                                    max_iter=10000)\r\n",
    "plot_estimator_scores(SVMEstimators,\r\n",
    "                        adjusted_parameter, adjusted_parameter_values,\r\n",
    "                        X_train_scaled, y_train, X_test_scaled, y_test, X_val_scaled, y_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Tree Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "adjusted_parameter = 'max_depth'\r\n",
    "adjusted_parameter_values = [1, 5, 10, 20, 50, 100]\r\n",
    "\r\n",
    "DecisionTreeEstimators = train_estimators(X_train_scaled, y_train,\r\n",
    "                                            DecisionTreeRegressor,\r\n",
    "                                            adjusted_parameter, adjusted_parameter_values,\r\n",
    "                                            splitter='random',\r\n",
    "                                            random_state=0)\r\n",
    "plot_estimator_scores(DecisionTreeEstimators,\r\n",
    "                        adjusted_parameter, adjusted_parameter_values,\r\n",
    "                        X_train_scaled, y_train, X_test_scaled, y_test, X_val_scaled, y_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "adjusted_parameter = 'max_depth'\r\n",
    "adjusted_parameter_values = [1, 5, 10, 20, 50, 100]\r\n",
    "\r\n",
    "RandomTreeEstimators = train_estimators(X_train_scaled, y_train,\r\n",
    "                                        RandomForestRegressor,\r\n",
    "                                        adjusted_parameter, adjusted_parameter_values,\r\n",
    "                                        random_state=0)\r\n",
    "plot_estimator_scores(RandomTreeEstimators,\r\n",
    "                        adjusted_parameter, adjusted_parameter_values,\r\n",
    "                        X_train_scaled, y_train, X_test_scaled, y_test, X_val_scaled, y_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-Nearest Neighbours Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "adjusted_parameter = 'weights'\r\n",
    "adjusted_parameter_values = ['uniform','distance']\r\n",
    "\r\n",
    "KNearestEstimators = train_estimators(X_train_scaled, y_train,\r\n",
    "                                        KNeighborsRegressor,\r\n",
    "                                        adjusted_parameter, adjusted_parameter_values,\r\n",
    "                                        n_neighbors=2)\r\n",
    "plot_estimator_scores(KNearestEstimators,\r\n",
    "                        adjusted_parameter, adjusted_parameter_values,\r\n",
    "                        X_train_scaled, y_train, X_test_scaled, y_test, X_val_scaled, y_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "adjusted_parameter = 'algorithm'\r\n",
    "adjusted_parameter_values = ['auto', 'ball_tree', 'kd_tree','brute']\r\n",
    "\r\n",
    "KNearestEstimators = train_estimators(X_train_scaled, y_train,\r\n",
    "                                        KNeighborsRegressor,\r\n",
    "                                        adjusted_parameter, adjusted_parameter_values,\r\n",
    "                                        n_neighbors=2)\r\n",
    "plot_estimator_scores(KNearestEstimators,\r\n",
    "                        adjusted_parameter, adjusted_parameter_values,\r\n",
    "                        X_train_scaled, y_train, X_test_scaled, y_test, X_val_scaled, y_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ada Boost Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "adjusted_parameter = 'n_estimators'\r\n",
    "adjusted_parameter_values = [10, 50, 100, 500, 1000, 5000]\r\n",
    "\r\n",
    "AdaBoostEstimators = train_estimators(X_train_scaled, y_train,\r\n",
    "                                        AdaBoostRegressor,\r\n",
    "                                        adjusted_parameter, adjusted_parameter_values,\r\n",
    "                                        random_state=0)\r\n",
    "plot_estimator_scores(AdaBoostEstimators,\r\n",
    "                        adjusted_parameter, adjusted_parameter_values,\r\n",
    "                        X_train_scaled, y_train, X_test_scaled, y_test, X_val_scaled, y_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gaussian Naive Bayes Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gaussian_nb = GaussianProcessRegressor()\r\n",
    "NaiveBayesEstimator = gaussian_nb.fit(X_train, y_train)\r\n",
    "gaussian_nb_train_score = NaiveBayesEstimator.score(X_train, y_train)\r\n",
    "gaussian_nb_test_score =  NaiveBayesEstimator.score(X_test, y_test)\r\n",
    "gaussian_nb_val_score =  NaiveBayesEstimator.score(X_val, y_val)\r\n",
    "print(f'{gaussian_nb_train_score=}, {gaussian_nb_val_score=}, {gaussian_nb_test_score=}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network Classification Through a Multi-Layer Perceptron"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(), activation='relu',\r\n",
    "                                            batch_size=100, max_iter=1000, learning_rate_init=0.01,\r\n",
    "                                            momentum=0.4, random_state=0, verbose=False).fit(X_train, y_train)\r\n",
    "\r\n",
    "print(f'Training Score: {mlp.score(X_train, y_train)}')\r\n",
    "print(f'Testing Score: {mlp.score(X_val, y_val)}')\r\n",
    "print(f'Validation Score: {mlp.score(X_test, y_test)}')\r\n",
    "\r\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(16), activation='relu',\r\n",
    "                                            batch_size=100, max_iter=1000, learning_rate_init=0.01,\r\n",
    "                                            momentum=0.4, random_state=0, verbose=False).fit(X_train, y_train)\r\n",
    "\r\n",
    "print(f'Training Score: {mlp.score(X_train, y_train)}')\r\n",
    "print(f'Testing Score: {mlp.score(X_val, y_val)}')\r\n",
    "print(f'Validation Score: {mlp.score(X_test, y_test)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Score: 0.40768464053522635\n",
      "Testing Score: 0.4075889111463352\n",
      "Validation Score: 0.4023350285392294\n",
      "Training Score: 0.9725842324654981\n",
      "Testing Score: 0.9716293756350687\n",
      "Validation Score: 0.9725405159352204\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('comp432': conda)"
  },
  "interpreter": {
   "hash": "6151f332f26dcde9b4e9a2d5bb7904271ea54c2018786e9b563d2c471a54dc4f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}